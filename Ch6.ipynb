{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subset Selection - identifying which of the p predictors is most related to the response\n",
    "\n",
    "Shrinkage - helps reduce variation by shrinking coeffients towards zero.  Some coefficients may be shrunk to 0, so can have similar effects to Subset Selection.  Ridge Regression and Lasso.\n",
    "\n",
    "Dimension Reduction - project p predictors into M-dimensional subspace where M < p.  Find important combinations of variables.  PCR, Partial Least Squares\n",
    "\n",
    "\n",
    "Subset selection options:\n",
    "\n",
    "Best Subset selection - brute force building all models with 1, 2 ... p predictors and choosing the best model at k predictors.  Use RSS (Residual Sum of Squares) and R^2 to identify the error rate of the model.  Have to be careful choosing which k to use, because larger k values will always have equal or lower RSS.\n",
    "\n",
    "R^2 is proportion of variance explained by a linear regression model, least squares model\n",
    "\n",
    "6.1.2 Stepwise Selection\n",
    "Similar to best subset selection, but at each level we choose the best model from the previous level, rather than all models from the previous level.\n",
    "\n",
    "Hybrid stepwise selection\n",
    "Can remove variables that no longer provide improvement.\n",
    "\n",
    "6.1.3 Choosing the optimal model\n",
    "\n",
    "R^2 and RSS estimate training error, but don't estimate test error, we need other ways to measure test error.  We can reserve a subset of the training data as test (Ch. 5).  We can also adjust the training error to account for bias due to overfitting\n",
    "\n",
    "$C_p$, AIC (Akaike information criterion), BIC (Bayseian information criterion), Adjusted $R^2$\n",
    "\n",
    "For a fitted least squares model containing $d$ predictors, the $C_p$ estimate of test MSE is:\n",
    "\n",
    "$C_p = \\frac{1}{n}(RSS + 2d\\hat{\\sigma}^2)$\n",
    "\n",
    "Note: $\\sigma^2 = Var(X)$ and $\\hat{\\sigma}^2$ is the estimate of the variance.  $d$ is the number of predictors.  $n$ is the number of observations.\n",
    "\n",
    "as $d$ increases, the penalty increases, to represent more likely overfitting.\n",
    "\n",
    "AIC\n",
    "\n",
    "$AIC = \\frac{1}{n\\hat{\\sigma}^2}(RSS + 2d\\hat{\\sigma}^2)$\n",
    "\n",
    "AIC is proportional to $C_p$.  BIC is slightly different.  BIC further penalizes large number of variables, so ofter results in smaller # of variables than $C_p$.\n",
    "\n",
    "$BIC = \\frac{1}{n\\hat{\\sigma}^2}(RSS + \\log(n)d\\hat{\\sigma}^2)$\n",
    "\n",
    "Adjusted $R^2$\n",
    "\n",
    "$R^2 = 1 - RSS/TSS$\n",
    "\n",
    "TSS (Total Sum of Squares)\n",
    "\n",
    "$TSS = \\sum{(y_i - \\bar{y})^2}$\n",
    "\n",
    "RSS always decreases as more variables are added, so $R^2$ always increases as more variables are added.\n",
    "\n",
    "$Adj. R^2 = 1 - \\frac{RSS/(n - d - 1)}{TSS/(n -1)}$\n",
    "\n",
    "Unlike other options, a large $Adj. R^2$ indicates a small test error.\n",
    "\n",
    "6.2 Shrinkage\n",
    "\n",
    "$l_2 norm = ||\\beta_1..\\beta_p||_2 = \\sqrt{\\sum{\\beta^2}}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. \n",
    "  1. (a) best subset should have smallest training RSS\n",
    "  2. (b) stepwise should have smallest test RSS\n",
    "  3. (c)\n",
    "    1. true\n",
    "    2. false\n",
    "    3. true\n",
    "    4. true\n",
    "    5. true\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.\n",
    "\n",
    "1. (a) C, lasso is less flexible than least squares, leading to decreased variance, but increased bias.\n",
    "2. (b) C, same as lasso because it restricts the size of the regression coeffiecients.\n",
    "3. (c) A, Non-linear methods are more flexible than linear regression, but can overfit test data and can better handle increases in variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. \n",
    "\n",
    "1. (a). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [1] \"AtBat\"     \"Hits\"      \"HmRun\"     \"Runs\"      \"RBI\"       \"Walks\"    \n",
      " [7] \"Years\"     \"CAtBat\"    \"CHits\"     \"CHmRun\"    \"CRuns\"     \"CRBI\"     \n",
      "[13] \"CWalks\"    \"League\"    \"Division\"  \"PutOuts\"   \"Assists\"   \"Errors\"   \n",
      "[19] \"Salary\"    \"NewLeague\"\n"
     ]
    }
   ],
   "source": [
    "library(ISLR)\n",
    "# fix(Hitters)\n",
    "print(names(Hitters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error in `colnames<-`(`*tmp*`, value = make.names(np)): attempt to set 'colnames' on an object with less than two dimensions\n",
     "output_type": "error",
     "traceback": [
      "Error in `colnames<-`(`*tmp*`, value = make.names(np)): attempt to set 'colnames' on an object with less than two dimensions\nTraceback:\n",
      "1. regsubsets(Y ~ ., data = data.full, nvmax = 10)",
      "2. regsubsets.formula(Y ~ ., data = data.full, nvmax = 10)",
      "3. leaps.setup(x, y, wt = wt, nbest = nbest, nvmax = nvmax, force.in = force.in, \n .     force.out = force.out, intercept = intercept)",
      "4. `colnames<-`(`*tmp*`, value = make.names(np))",
      "5. stop(\"attempt to set 'colnames' on an object with less than two dimensions\")"
     ]
    }
   ],
   "source": [
    "library(leaps)\n",
    "set.seed(1)\n",
    "X <- rnorm(100)\n",
    "noise <- rnorm(100)\n",
    "# b)                                                                                                                                \n",
    "Y <- 1+2*X+3*X^2+3*X^3+noise\n",
    "# X <- poly(X,10)\n",
    "# c)                                                                                                                                \n",
    "data.full <- data.frame(X, Y)\n",
    "regfit <- regsubsets(Y~.,data=data.full, nvmax=10)\n",
    "par(mfrow=c(2,2))\n",
    "plot(regfit, scale=\"adjr2\")\n",
    "plot(regfit, scale=\"Cp\")\n",
    "plot(regfit, scale=\"bic\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
