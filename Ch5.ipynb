{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross validation and bootstrap.\n",
    "\n",
    "Training error vs test error\n",
    "Training error is the error we get from applying the model to the data we used to build the model.\n",
    "Test error is the error we get from applying the model to data that has not been used to build the model.\n",
    "\n",
    "CP statistic AIC, BIC, use to estimate test error from training error.\n",
    "\n",
    "Cross-validation, hold out some part of data from training set to use as test data.  Measured by MSE (mean squared error) for quantitative responses and misclassification error rate for qualitative responses (classifications)\n",
    "\n",
    "CV is used to pick polynomial of the model, not good at telling us the level of the error, wide range.  Highly variable because we're splitting into 2 parts, throwing away half the data in training.\n",
    "\n",
    "Generally more observations lead to lower error rate.\n",
    "\n",
    "5.1.2 LOOCV (Leave One Out Cross Validation)\n",
    "make multiple training sets, each with one value less in it.  Train and test each successive subset and average the MSE across all the permutations of training/test sets.\n",
    "\n",
    "Advantage is less bias in the result, and no randomness.  Disadvantage is that it can be expensive to implement\n",
    "\n",
    "5.1.3 k-fold cross validation\n",
    "Same as LOOCV, but you take $n/k$ observations as test, with the remainder as training.  You take successive subsets and average the MSE across the permutations of training/test sets. LOOCV is basically k-fold with k set to n.  However, one usually uses k=5 or k=10, mostly for computational advantage.\n",
    "\n",
    "5.1.4 bias-variance trade-off for k-fold CV vs LOOCV\n",
    "\n",
    "5.1.5 CV on classification problems\n",
    "Instead of using MSE, calculated avg # of misclassified observations per subset.\n",
    "\n",
    "5.2 the bootstrap\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 1. \n",
    "Using basic statistical properties of the variance, as well as single-variable calculus, derive (5.6). In other words, prove that $α$ given by (5.6) does indeed minimize $Var(αX + (1 − α)Y )$.\n",
    "\n",
    "(5.6)\n",
    "$α= \\frac{\\sigma_Y^2 − σ_{XY}}{σ_X^2 + σ_Y^2 − 2σ_{XY}}$\n",
    "\n",
    "where $σ_X^2 = Var(X)$, $σ_Y^2 = Var(Y)$, and $σ_{XY} = Cov(X, Y)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. \n",
    "We will now derive the probability that a given observation is part of a bootstrap sample. Suppose that we obtain a bootstrap sample from a set of n observations.\n",
    "\n",
    "(a) What is the probability that the first bootstrap observation is not the jth observation from the original sample? Justify your answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
